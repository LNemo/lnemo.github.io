---
layout: post
title: "[부스트캠프 AI Tech 8기] Day 31: 경진대회 준비, 텍스트 데이터 탐색과 전처리"
date: 2025-10-20 18:30:00+0900
categories: [boostcamp]
tags: [boostcamp, ai, nlp, preprocessing, competition]
description: "자연어처리 모델 학습 파이프라인을 이해하자."
keywords: [colab, nlp, eda, feature engineering, data, data preprocessing, bert, word2vec]
image:
  path: /assets/img/posts/boostcamp/boostcamp.jpg
  alt: "네이버 부스트캠프"
comment: true
---

# 머신러닝 대회 준비를 하자

머신러닝 경진대회를 참가한다는 것은 너무 막연하게만 느껴집니다. 대회를 참가할 때에 어떤 흐름으로 참가할 수 있는지 확인해봅시다. 아래는 키워드 중심으로 정리하였습니다.

## 머신러닝 경진대회 입문
### EDA
* 데이터 품질 평가 및 변수 분석
  * 데이터 품질 평가
    * 결측값 탐색
    * 이상값 탐색
  * 변수 간 관계 분석
    * 변수 간 상솬관계 분석
    * 변수 간 관계 시각화
  * 모델링 방향성 설정
    * 주요 인사이트 도출
    * 데이터 전처리 계획

### Feature Engineering
* 중요성
  * 변수 표현을 향상시켜 모델 예측력 향상
  * 데이터 변환을 통한 모델 학습의 용이화
  * 데이터 정제를 통한 모델 안정성 향상
* 주요 작업
  * 새로운 변수 생성
  * 기존 변수 변형
  * 데이터 전처리(결측값 처리, 이상치 수정 등)

### 모델 선택
* No Free Lunch Theorem
  * 모든 머신러닝 알고리즘이 모든 문제에서 동일한 성능을 보일 수 없다
  * 데이터 분포, 크기, 변수의 종류에 따라 모델 선택이 달라질 수 있다
* 문제 유형
  * 분류 Classification
  * 회귀 Regression
  * 클러스터링 Clustering
* 정형 데이터를 다룬다면? → 딥러닝 모델보다 좋은 트리 기반 모델
  * 높은 예측 성능 (특히 데이터가 적거나 feature 수가 적을 때)
  * 유연한 하이퍼파라미터 조정
  * 해석 용이성 (특징 중요도 등 제공)
  * 빠른 학습 및 추론 속도
  * ex) Random Forest, XGBoost, LightGBM, CatBoost
* 이미지, 텍스트, 오디오 같은 비정형 데이터라면? → 딥러닝 기반 모델
  * 입력 데이터의 복잡한 패턴 인식에 강한 성능
  * 고차원 표현을 자동으로 학습하는 능력
  * Pretrained 모델을 활용한 전이 학습(Transfer Learning) → 빠른 성능 향상
  * ex) VIT, Transformer, WaveNet
* 실험 정신
  * 다양한 모델을 직접 실험해보고, 그 결과를 바탕으로 최적의 모델을 선택하는 것이 중요

### 모델 검증 프로세스
**대회의 목표는 “과적합 되지 않은 일반화 된 모델”**
* 검증 프로세스 구축 단계
  * 검증 데이터 분할
  * 교차 검증
  * 데이터 누수 방지
  * 균형 잡힌 데이터 분할
* 검증 프로세스에서 흔히 하는 실수
  * 전처리 시에 **검증 데이터를 Unseen 한 상태**를 보장하지 못하는 경우
  * **과적합을 방지**할 방법을 고려하지 않은 경우
  * 적절하지 않은 **평가 지표**를 선택한 경우

**더 높은 성능을 위한 프로세스**
- 하이퍼 파라미터 튜닝
- 앙상블 — 학습된 모델 여러 개의 예측 값을 혼합하여 더 좋은 결과를 만들어내는 기법. 단순히 예측 값을 평균내거나 섞는 것만으로도 성능 향상 기대할 수 있음
### 경진대회를 대하는 마음가짐
* 점수 이상의 성과를 추구하기 - 점수보다 중요한 것이 아니라 내가 얼마나 성장하는 것이 중요
  * 나보다 잘하는 사람은 어떻게 했는데 잘할 수 있었는지 조언받기
* 성장이란 무엇인지?
  * 과거의 나와 현재의 나를 비교해 보는 것
  * 과거의 나와 비교한 것을 작성해놓으면 좋을 듯

## 텍스트 데이터 탐색과 전처리

### 정형 데이터와 비정형 데이터  

| **구분**  | **정형 데이터**(Structured Data) | **비정형 데이터**(Unstructured Data)   |
|:-------:|:---------------------------:|:--------------------------------:|
| 구조      | 사전에 정의된 명확한 스키마             | 스키마가 없거나 유연함                     |
| 저장 방식   | 관계형 데이터베이스                  | NoSQL, Data Lake, 객체 스토리지        |
| 분석 및 처리 | SQL, BI 툴을 통한 직접 분석         | NLP, 컴퓨터 비전 등 복잡한 전처리 및 AI 모델 필요 |
| 검색 용이성  | 매우 쉬움 (정확, 빠름)              | 어려움 (내용 기반 검색에 복잡한 기술 요구)        |
| 데이터의 양  | 전체 데이터의 약 20%               | 전체 데이터의 약 80% 이상을 차지하며 빠르게 증가    |
| 대표적인 예시 | 판매 기록, 재무 데이터, ERP/CRM 데이터  | 소셜 미디어 게시물, 동영상, 오디오 문서 파일       |

- 비정형 데이터에서.. 
  비전 vs. 텍스트  

| **구분** | **비전 데이터** | **텍스트 데이터** |
|:-:|:-:|:-:|
| 기본 단위 | 픽셀 | 단어, 형태소 |
| 구조 | 고정된 픽셀 그리드 | 구조 없음, 가변적 길이 |
| 정보의 종류 | 공간적(Spatial) 정보가 중요 | 순차적(Sequential) 정보가 중요 |
| 의미 해석 | 시각적 패턴 (객관적) | 문맥에 따라 의미 변화 (중의적, 주관적) |

### 텍스트 데이터의 특징과 처리의 어려움
#### 형태적 비정형성: 노이즈
> \<p\>이 영화 강추!!! ㅋㅋㅋ 별점 5점 드려요. ★★★ 제 블로그에 리뷰 올려놨어요! ~[https://blog.com/1234/](https://blog.com/1234/)~ #영화 \</p\>

* \<p\> 같은 html 태그는 실제 데이터와 무관
* !, ★, # 등의 특수문자는 감정 표현이나 기호로 사용되지만 일반적인 분석에는 방해가 될 수 있음
* "ㅋㅋㅋ" 같이 반복되는 자음이나 모음은 'ㅋ'라는 하나의 문자로 정규화 가능
* URL은 외부 링크 정보이므로 텍스트 의미 분석에는 불필요

#### 의미의 복잡성 및 모호성
* 문맥 의존성
  * 다의어 & 동음이의어
  * “건너편으로 가는 다리를 건넜다.” vs. “친구 사이에 다리를 놓아주었다.”
  * “맛있는 사과를 먹었다.” vs. “진심 어린 사과를 건넸다.”
* 표현의 모호성
  * ‘사과’, ‘apple’, ‘apples’는 모두 같은 과일을 의미하지만 컴퓨터에게는 완전히 다른 세 개의 문자열이다.

#### 구조적 의존성
* “I saw a man with a telescope.”
  * 단어의 관계에 따라 다른 의미를 가질 수 있음
  * 내가 telescope를 통해 a man을 본 것
  * 내가 telescope를 지니고 있는 a man을 본 것

### 자연어 분류 모델링에서 전처리의 역할과 중요성
**GIGO**
* Garbage in, Garbage out
* 쓰레기 넣으면 쓰레기 나옴
* 데이터를 정제하고 가공하는 전처리는 모델의 성능을 좌우하기 때문에 선택이 아닌 필수임

**전처리 파이프라인 (Preprocessing Pipeline)**
* 원본 텍스트 → 정제 → 토큰화 → 표준화 → 벡터화 → 모델입력
* 절대적인 규칙은 아님,
* 모델의 예측 오류를 분석해서 전처리 규칙을 수정하는 **지속적인 개선**(Human-in-the-Loop)이 필요

#### 전처리 기대 효과
* 인사이트 발굴 및 데이터 이해도 증진
* 학습 효율 증대
* 일반화 능력 향상
* 성능 향상

### 텍스트 정제
#### 기본정제
* 소문자 변환
* 구두점 제거
* 불필요한 공백 제거

#### 정규 표현식
정규 표현식은 특정 패턴을 가진 문자열을 찾아내고 변경하는 강력한 도구
* HTML 태그 \<p\>, \<div\>
* URL, 이메일 주소 https://... , example@ex.com
* 해시태그 및 사용자 멘션 #AI #인공지능 , @김캠퍼
* 숫자 (분석 목적에 따라 모든 숫자를 제거하거나 특정 토큰으로 대체)
* 반복되는 문자 (’ㅋㅋㅋ’, ‘ㅠㅠ’, ‘진짜아아아’ 처럼 감정 표현을 위한 반복 자음/모음 등)

**모든 노이즈를 항상 제거하는 것이 정답은 아님**
* 문장부호나 이모티콘은 감정의 강도와 뉘앙스를 담고 있는 핵심 데이터
  * **특별한 토큰으로 치환**하거나 **그대로 유지**하는 전략을 고려할 수 있음
* 뉴스 기사에서 기업의 주가나 실적을 분석할 때에 숫자는 가장 핵심적인 정보임. 숫자를 제거하면 안됨
  ⇒ 정제를 시작하기 전에 “내가 이 분석을 통해 무엇을 얻고 싶은가?”를 고려하고 정제 규칙을 설계해야 함

### 텍스트의 구조화 및 수치화
토큰화는 컴퓨터가 읽을 수 있는 컴퓨터가 처리할 수 있는 의미있는 최소 단위(토큰)로 쪼개는 과정
#### 토큰의 단위
* 단어 word — 단순히 띄어쓰기를 기준으로 분리
* 형태소 morpheme — 의미를 갖는 가장 작은 단위로 분리. 조사를 분리하고 어간을 추출
* 서브워드 subword — 단어를 더 작은 단위로 분리. 신조어와 OOV 문제를 해결해 현재 AI 언어 모델의 표준

한국어 토큰화는 교착어 특성 때문에 띄어쓰기만으로 토큰화를 하면 학습 효율이 떨어짐
* 한국어 처리는 KoNLPy, Mecab 같은 형태소 분석기 사용이 중요

#### 표준화
AI 모델이 텍스트를 효율적으로 학습하고 더 정확하게 분석하게 만들기 위해 형태는 다르지만 비슷한 단어들을 하나의 기준으로 통일하는 과정
* 불용어 제거 — 분석에 불필요한 단어 제거 (영어는 a, the, is, in, on / 한국어는 은, 는, 이, 가, 을, 를) 주로 조사, 접속사, 어미 등
* 어휘 표준화 — 단어의 형태를 원형으로 통일
  * 어간 추출(Stemming) vs. 표제어 추출(Lemmatization)

| **구분** | **어간 추출**                    | **표제어 추출**                   |
|:------:|:----------------------------:|:----------------------------:|
| 핵심 원리  | 규칙 기반으로 **어미를 잘라냄**          | 사전과 문법을 이용해 **원형을 찾음**       |
| 정확도    | 낮음                           | 높음                           |
| 속도     | 빠름                           | 느림                           |
| 예시     | studies → studi<br>ran → ran | studies → study<br>ran → run |

### 벡터화
모델이 학습할 수 있도록 전처리가 완료된 토큰들을 숫자 벡터로 변환
* 텍스트에 있는 모든 고유한 토큰들을 모아 단어 사전(Vocabulary)을 만듦
* One-Hot Encoding vs. Bag-of-Words(BoW)
  * One-Hot Encoding은 표현하고 싶은 단어의 위치만 1, 나머지는 0으로 표시
  * BoW는 문장이나 문서 전체를 표현하는 방식. 문장에 포함된 단어들이 단어 사전에서 어디에 해당하는지 1로 표시하거나 단어의 등장 횟수를 직접 숫자로 기록 (단어의 순서가 무시됨)
* 고급 벡터화 방법: 단어의 의미를 벡터에 담기
  * TF-IDF(Term Frequency-Inverse Document Frequency)
    * BoW의 한계 보완
    * 문서 내 단어의 중요도를 계산하여 가중치 부여. 문서에 자주 등장하지만 다른 문서에서 잘 나오지 않는 단어에 높은 점수
  * Word2Vec & GloVe
    * 예측 기반 임베딩
    * “비슷한 문맥에서 등장하는 단어는 비슷한 의미를 가질 것”이라는 아이디어에서 출발
    * 단어의 의미를 밀집 벡터로 표현하여 단어 간 의미적 관계를 계산할 수 있음 (서울 - 한국 + 일본 ≈ 도쿄)
  * BERT & ELMo
    * 문맥 기반 임베딩
    * 최신 언어 모델들이 사용하는 방식. 문맥에 따라 단어의 의미가 달라지는 것을 반영
    * 동음이의어(사과 등)의 벡터를 문맥에 맞게 다르게 표현
