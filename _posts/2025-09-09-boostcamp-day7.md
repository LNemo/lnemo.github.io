---
layout: post
title: "[부스트캠프 AI Tech 8기] Day 7: Greadient Descent"
date: 2025-09-09 18:53:00+0900
categories: [boostcamp]
tags: [boostcamp, ai, 부스트캠프, numpy, gradient descent]
description: "AI Math에 대해서 배우자."
keywords: [numpy, ai math, tensor, colab, 행렬, 벡터, 텐서, einsum, einops, gradient, descent, 미분, weight, 기울기, 경사하강법]
image:
  path: /assets/img/posts/boostcamp/boostcamp.jpg
  alt: "네이버 부스트캠프"
comment: true
---

# Greadient Descent

## 미분

미분은 함수의 기울기를 구하는 과정입니다. 한 점에서의 기울기를 알면 함수의 최소값이 어디에 있는지 추측할 수 있습니다. 만약에 기울기가 양수라면 최소값으로 향하려면 그래프 상 왼쪽으로 가야할 것입니다. 반대로 기울기가 음수라면 그래프 상 오른쪽으로 가야합니다.

경사하강법은 이러한 점을 활용한 방법입니다. Loss 함수를 구하고 해당 loss가 최소가 되는 점을 구하는 과정입니다. 기울기가 양수일 때에 x좌표를 왼쪽으로 움직여야 하기 때문에 기울기만큼을 빼주면서 학습합니다. 기울기가 음수일 때에는 오른쪽으로 가기 위해서 기울기만큼을 빼주어서 학습합니다. 결국에는 이 미분값이 음수든 양수든 빼준다면 우리는 최소값으로 향한다는 것을 알 수 있습니다.

벡터는 숫자가 element인 data type입니다. **배열**이라고 불립니다. 벡터의 차원은 벡터의 원소의 개수를 말합니다. 

## 벡터의 미분

일반 다항식의 미분은 쉽게 할 수 있습니다. 하지만 벡터가 입력인 다변수 함수에 대해서는 미분하기 어렵습니다. 이 때 우리는 **편미분**(partial differentiation)을 사용할 수 있습니다.

함수를 각 변수 별로 편미분하여 계산한 값을 벡터로 나타낸 것을 그래디언트 벡터(Gradient vector)라고 합니다. ∇f 처럼 표기합니다. (역삼각형은 nabla)

다항식에서 경사하강법은 종료조건에 절대값을 사용했지만 벡터에서의 경사하강법은 norm 값을 사용합니다. 벡터의 경사하강법을 사용해서 선형회귀 알고리즘을 구현할 수 있습니다. 

이론적으로 경사하강법은 미분가능하며 볼록한(convex) 함수에 대해서는 수렴이 보장되어 있습니다. 하지만 비선형회귀 문제에서는 목적식이 볼록하지 않을 수 있으므로 수렴이 항상 보장되지는 않습니다. 또한 경사하강법은 극값에 도달하면 더이상 업데이트를 하지 못하기 때문에 그래디언트 벡터가 0 근처로 떨어지게 된다면 학습이 거의 되지 않습니다. 특히 딥러닝처럼 손실함수가 비볼록함수인 경우에 더 심해질 수 있습니다.

## 확률적 경사하강법

확률적 경사하강법(Stochastic Gradient Descent)는 데이터를 한 개 또는 일부를 활용하여 weight를 업데이트 하는 방법입니다. SGD는 **볼록하지 않은 함수도 최적화** 할 수 있으며(기계학습 모델도 가능) 메모리 차원에서도 **더 효율적인 계산**을 할 수 있습니다.

SGD를 학습할 때에는 학습률을 고정시키지 않고 스케줄러를 사용하여 학습률을 step마다 조정합니다. 처음에는 목표로 하는 값과 멀리 떨어져 있기 때문에 큰 학습률을 적용하고 학습 횟수가 늘어날수록 미세조정을 하기 위해 천천히 움직이도록 작은 학습률을 적용하도록 합니다. (``lr(t, init=1.): return init / (t+1)``)

---
**피어세션을 통해 알아간 것** - Seq2Seq
- Seq2Seq에서 beam size는 무엇을 의미하는지
  - beam search는 먼저 나온 단어와 현재 예측할 단어를 조합해서 확률을 계산하게 되는데 해당 조합들이 저장되는 size
- Seq2Seq에서는 one-hot 벡터가 아니라 임베딩 벡터가 사용되었다
  - 단어에 여러 특징 변수들을 두는 벡터
- 5개의 앙상블이 사용되었다는 것은 하나의 모델만 사용한 것이 아니라 독립적으로 초기값을 다르게 해서 개별적으로 모델들을 학습시킨 것
  - 추론시에는 모델이 생성한 후보들의 확률 평균을 내는 방식을 사용
- Seq2Seq의 LSTM은 4개의 레이어 사용

