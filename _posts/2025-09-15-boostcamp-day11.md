---
layout: post
title: "[부스트캠프 AI Tech 8기] Day 11: ML LifeCycle 기초"
date: 2025-09-15 18:50:00+0900
categories: [boostcamp]
tags: [boostcamp, ai, ml, 머신러닝, 선형대수]
description: "머신러닝의 생애주기에 대해 배우자."
keywords: [numpy, colab, ML LifeCycle, regression, nn classifier, linear classifier]
image:
  path: /assets/img/posts/boostcamp/boostcamp.jpg
  alt: "네이버 부스트캠프"
comment: true
---

# ML LifeCycle 기초

## 머신러닝

머신러닝(Machine Learning, ML)은 다음의 알고리즘을 연구하는 학문입니다.
- **어떠한 작업 T**에 대해
- **경험 E**와 함께
- **성능 P**를 향상시킨다.
예시로 이미지 분류 작업을 머신 러닝으로 구현한다고 하면, 주어진 이미지가 무엇인지 분류하도록(T) 다양한 이미지 데이터들(E)을 활용해 이미지 분류 정확도(P)를 높이는 것입니다.

### 머신러닝 종류
머신러닝에는 지도 학습(supervised learning), 비지도 학습(unsupervised learning), 강화 학습(reinforcement learning)이 있습니다.

#### 지도 학습(Supervised Learning)
지도학습은 귀납적 학습으로 입력 데이터와 원하는 출력을 세트로 모델에 제공해서 학습하도록 하는 방법입니다. 주로 **회귀 문제**에 사용됩니다. 

#### 비지도 학습(Unsupervised Learning)
비지도 학습은 입력 데이터만이 주어집니다. 라벨이 없기 때문에 주어진 데이터 뒤에 숨겨진 구조를 출력합니다. 주로 **Clustering**에 사용됩니다.

#### 강화 학습(Reinforcement Learning)
강화학습은 보상이 있는 상태와 액션의 시퀀스가 주어졌을 때, 보상이 있는 일련의 상태와 작업이 주어지면 policy를 출력합니다. Policy는 주어진 상태에서 수행해야 할 작업을 알려주는 ‘상태 -> 작업’ 형식의 매핑입니다. 주로 **게임플레이, 로봇 공학, 자율 주행** 등에 사용됩니다.

### ML LifeCycle
머신러닝 라이프 사이클은 머신러닝 모델을 개발, 배포, 유지보수 하는 일련의 단계들을 정의하는 프로세스입니다.

1. 데이터 엔지니어가 라벨링한 데이터를
2. 데이터 사이언티스트가 훈련 코드를 사용해 모델을 생성하고
3. 해당 코드를 ML 엔지니어, 개발자가 웹 앱코드에 탑재하고 배포합니다,

위의 프로세스를 자세히 구분한 머신러닝 라이프 사이클은 다음으로 구성됩니다:

1. 계획하기(Plaaning)
2. 데이터 준비(Data Preparation)
3. 모델 엔지니어링(Model Engineering)
4. 모델 평가(Model Evaluation)
5. 모델 배포(Model Deployment)
6. 모니터링 및 유지, 관리(Monitoring and Maintenance)

## Regression and NN Classifier


### 선형 회귀

회귀 분석은 관찰된 연속형 변수들에 대해 두 변수 사이의 모형을 구한 뒤 적합도를 측정하는 분석 방법입니다. 회귀 분석에서 사용되는 변수는 ‘독립 변수’와 ‘종속 변수’가 있습니다. 쉽게 이해하자면 독립변수는 input이고 종속 변수는 output입니다.

선형 회귀를 위해서는 다음의 가정을 만족하여야 합니다:
- **선형성**: 독립 변수와 종송 변수의 관계는 선형적
- **독립성**: 관측값들은 서로 독립 (관측치들 간의 상관관계가 없어야 함)
- **등분산성**: 오류의 분산이 일정 (특정 구간에서 잔차의 분산이 커지거나 작아지면 등분산성 가정에 위배)
- **정규성**: 오류가 정규 분포를 따름


선형 회귀에서는 최적화 방법 중 주로 **최소 제곱법**(OLS)을 사용합니다. 최소 제곱법은 관측값과 예측값의 차이인 잔차의 제곱합을 최소화하는 매개변수를 찾는 것입니다. 

**다중 선형 회귀**는 여러 독립 변수를 포함하도록 선형 회귀를 확장한 것입니다. 

모델 평가 지표에는 다음의 방법이 있습니다:
- 평균 절대 오차(MAE)
- 평균 제곱 오차(MSE)
- 제곱근 평균 제곱 오차(RMSE): MSE를 제곱근 한 것
- 결정 계수(R²): 모델이 종속 변수의 변동성을 얼마나 설명하는지 나타내는 지표
  - 0~1 사이의 값을 가지며 1에 가까울수록 모델이 데이터를 잘 설명하는 것
  - R² = 1 - (SSR, 잔차 제곱 합)/(SST총 제곱 합)

### Nearest Neighbor(NN) Classifier

NN Classifier는 가장 가까운 데이터를 기준으로 class를 분류합니다. k=1이라면 가장 가까운 데이터 하나를 기준으로 분류하고, k=3이라면 가장 가까운 데이터 3개를 기준으로 분류합니다. 예를 들어, 가장 가까운 데이터 3개가 빨강, 빨강, 노랑이라면 해당 데이터는 빨강으로 분류됩니다.

거리는 L1 또는 L2로 구할 수 있습니다. L1은 단순히 빼서 절대값을 취한 값들의 합이고, L2는 뺀 값을 제곱하여 더한 것을 제곱근 한 값입니다.

해당 Classifier는 훈련 과정은 빠르지만 예측 과정이 느립니다. 

이미지를 분류하는 모델을 만든다고 할 때, 훈련에는 해당 모델에 단순히 모든 이미지를 넣습니다. 하지만 예측할 때에는 모든 이미지와 예측하려는 이미지의 거리를 일일이 계산하여 비교하여야 합니다. 그렇기 때문에 데이터가 커질수록 예측시간이 그만큼 더 커진다는 치명적인 단점이 있습니다. (훈련 시에는 ``O(1)``, 예측 시에는 ``O(N)``)

또한 차원이 커질수록 기하급수적으로 증사하는 예제 수가 필요합니다.

## Linear Classifier

NN Classifier는 모든 데이터와 비교해야하기 때문에 예측에 시간이 너무 오래 걸린다는 단점이 존재합니다. 그럼 모든 데이터를 모두 외우는 대신에 입력을 레이블에 매핑하는 함수(f)가 있다고 합시다.

고양이 사진을 f에 넣는다면 ‘고양이’라고 분류하고, 강아지 사진을을 f에 넣는다면 ‘강아지’라고 분류할 수 있는 이 함수를 만들 수 있다면 모든 이미지들과 비교하지 않아도 되므로 예측 시간이 매우 단축될 것입니다.

일단은 간단하게 이 함수가 선형 함수라고 생각해봅시다. ``x``가 데이터, ``W``가 가중치라고 생각했을 때, ``Wx``라고 표현할 수 있습니다. 여기에 간단하게 편향(bias, y절편)을 추가합니다. 그렇다면 ``f(x, W) = Wx+b``로 나타낼 수 있습니다. 이런 식으로 우리는 가중치와 편향만 모델에 기억한다면 **모든 데이터를 저장할 필요도 없고** **빠른 예측**도 가능합니다. 이렇게 선형적으로 나타낸 classifier를 Linear Classifier라고 합니다.

각 선형 바운더리는 ``W``의 해당 행에서 나옵니다. 각 행에서의 가중치로 해당 label인지 score를 계산합니다! 따라서 ``W``의 행의 크기가 label 수와 같다는 것을 알 수 있습니다. 편향은 해당 바운더리를 위 아래로 이동시키는 역할을 합니다.

하지만 이렇게 계산된 점수로는 어떠한 판단을 내리기 힘든 것 같습니다. ``124``가 얼마나 높은 점수인지 ``42.4``는 또 얼마나 높은 점수인지 단순 수로써는 알 수 있는 방법이 없습니다. 따라서 softmax를 사용해 0과 1사이의 확률로 표시합니다(이진 분류라면 sigmoid). 

---
**피어세션을 통해 알아간 것** - attention
- Attention에서 벡터를 그대로 사용하지 않고 가중합을 사용하는 이유
  - 해당 확률을 사용해서 디코더에 사용하려고 만든 벡터(context vector, c_t)이기 때문에 이미 벡터를 만들 때에 사용함
- 예전에는 ‘NULL’ 토큰이 필요했던 이유
  - SMT 모델에서는 1:1로 짝지었기 때문에 항상 1:1로 대응하지 않던 언어 특성 상 NULL과 짝지어야 하는 경우가 있었음
- Teacher Forcing이란?
  - 잘못된 예측을 통해 다음 토큰 예측을 하게되면 계속 잘못된 결과가 나올 수 있음
  - 따라서 teacher forching rate에 따라 실제 정답을 입력으로 넣거나, 모델의 출력을 입력으로 넣어 학습을 진행하는 기법
- 
---
- Cross-Entropy는 모든 샘플에 대한 ``-log("올바른 클래스에 대한 예측 확률")``의 합계 
  - 확률이 1에 가까워지면 손실이 0에 수렴하고 0에 가까워지면 손실이 매우 크게 증가
