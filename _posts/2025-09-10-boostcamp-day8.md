---
layout: post
title: "[부스트캠프 AI Tech 8기] Day 8: 확률론과 통계학"
date: 2025-09-10 19:13:00+0900
categories: [boostcamp]
tags: [boostcamp, ai, numpy, 확률론, 통계학]
description: "AI Math에 대해서 배우자."
keywords: [numpy, ai math, tensor, colab, 경사하강법, 조건부확률, 몬테카를로, 기댓값, 베이즈정리, 조건부확률, likelihood, 가능도, 확률분포]
image:
  path: /assets/img/posts/boostcamp/boostcamp.jpg
  alt: "네이버 부스트캠프"
comment: true
---

# 확률론과 통계학

## 확률론

확률분포는 데이터의 초상화입니다. 이산형 확률변수는 확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링하고, 연속형 확률변수는 데이터 공간에 정의된 확률변수의 밀도(density) 위에서의 적분을 통해 모델링합니다.

### 베이즈 정리

베이즈 정리는 조건부 확률을 통해 정보를 업데이트 하는 방법을 알려줍니다. 

![이미지](/assets/img/posts/boostcamp/day8/bayesrule.png)

위 사진에서 **P(b\|a)**는 사후확률, **P(b)**는 사전확률, **P(a\|b)**는 가능도, **P(a)**는 evidence(주변확률)입니다.

### 기댓값

**기댓값**(Expectation)은 **어떤 확률 과정을 무한히 반복했을 때, 얻을 수 있는 값의 평균으로서 기대할 수 있는 값**으로 데이터를 대표하는 통계량입니다. 확률분포를 통해 다른 통계적 범함수를 계산하는데에도 사용됩니다.

![이미지](/assets/img/posts/boostcamp/day8/expectation.png)

### 대수의 법칙

대수의 법칙은 데이터를 확률분포로부터 반복적으로 독립추출할 때 산술평균이 기댓값으로 거의 수렴하는 것을 의미합니다. 대수의 법칙은 데이터가 확률분포와 상관없이 기댓값을 게산할 수 있는 통계량이라면 성립합니다. 기댓값이 발산하거나 정의되지 않는다면 성립하지 않습니다. 

**몬테카를로 샘플링**은 대수의 법칙을 사용한 방법입니다. 확률분포를 모르거나 적분값을 구할 수 없을 때에 이 방법을 사용할 수 있습니다. 아래는 몬테카를로 샘플링을 사용하여 원주율을 구하는 과정의 사진입니다.

![이미지](/assets/img/posts/boostcamp/day8/monte.png)

## 통계학

**통적 모델링**은 **적절한 가정 위에서 확률분포를 추정하는 기법**입니다. 데이터가 특정 확률분포를 따른다고 선험적으로 가정하고 분포를 추정하는 방법을 모수적 방법론이라고 합니다.

데이터가 아래의 특징을 가지면 특정 확률분포로 모델링할 수 있습니다.
- 데이터가 2개의 값(0, 1)만 가지는 경우 -> 베르누이 분포
- 데이터가 n개의 이산적인 값을 가지는 경우 -> 카테고리 분포
- 데이터가 [0, 1] 사이에서 값을 가지는 경우 -> 베타 분포
- 데이터가 0 이상의 값을 가지는 경우 -> 감마 분포, 로그정규 분포 등
- 데이터가 R 전체에서 값을 가지는 경우 -> 정규 분포, 라플라스 분포 등

특정 확률 분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀌면 비모수 방법론이라고 부릅니다. 기계학습의 상당수 방법론은 비모수 방법론에 속합니다.

### 최대가능도 추정법

통계적 기계학습에서 가장 많이 사용되는 모델 학습 원리 중 하나는 최대가능도 추정법(Maximum Likelihood Estimation, MLE)입니다. Likelihood는 모수를 따르는 분포에서 데이터 x를 관찰할 가능성을 뜻합니다. 가능도를 구할 때에 모든 확률을 곱하는 이유는 해당 데이터의 사건들이 동시에 일어났을 때의 모수를 구하는 것이기 때문입니다. 

![이미지](/assets/img/posts/boostcamp/day8/mle.png)

가능도에 로그를 씌우고 최적화를 한다면 곱셈을 덧셈으로 바꾸어 연산이 가능합니다. 이때 경사하강법으로 최적화할 때에 연산량을 O(n^2)에서 O(n)으로 줄일 수 있습니다. 

### KL Divergence

쿨백-라이블러 발산(Kullback-Leibler Divergence)은 확률분포 사이의 거리를 계산할 때 사용됩니다. KL 발산을 알아보기 전에 엔트로피에 대해서 알 필요가 있습니다.

#### 엔트로피

엔트로피는 **불확실성** 또는 **평균 정보량**을 숫자로 나타낸 것입니다. 

A 도시: 1년 중 99%가 맑음, 1%가 흐림  
B 도시: 1년 중 50%가 맑음, 50%가 흐림

위 도시들의 경우 A 도시는 엔트로피가 작고 B 도시는 엔트로피가 크다고 할 수 있습니다. 우리는 예측하기 어려운 것, 불확실성이 높은 것을 엔트로피가 높다고 합니다.

평균 정보량이라는 표현은 와닿지 않을 수 있습니다. A 도시와 B 도시의 날씨 정보를 표현한다고 할 때 통신 비용을 줄이기 위해서 메시지의 길이를 짧게 만든다고 합시다. B의 도시는 맑고 흐릴 확률이 각각 50%이므로 맑음을 ``0``, 흐림을 ``1``로 표현할 수 있습니다. 반면 A 도시의 경우는 대부분의 경우 맑기 때문에 맑음을 ``0``, 흐림을 ``1111110``로 표현하게 됩니다(실제로는 맑음일때 더 적은 평균 비트수인 0.014 비트가 부여됨). 높은 확률을 가진 날씨가 더 작게 표현되고 낮은 확률의 날씨가 길게 표현된다면 더 효율적으로 메시지를 보낼 수 있는 것입니다.

그렇게 계산한다면 각각의 엔트로피, 평균 비트수는

``A 도시의 엔트로피``  
`` = (0.99 × 0.014 비트) + (0.01 × 6.64 비트) ≈ 0.014 + 0.066 = 0.08 비트``  
`` B 도시의 엔트로피``  
`` = (0.5 × 1 비트) + (0.5 × 1 비트) = 0.5 + 0.5 = 1 비트``

인 것을 알 수 있습니다.

#### 크로스-엔트로피

크로스-엔트로피는 예측 분포를 실제 분포에 적용 시켰을 때에 필요한 평균 정보량을 의미합니다. 

A 도시에 예보관이 있습니다. 이 예보관은 이 도시가 70% 확률로 맑고 30% 확률로 흐릴 것이라고 생각했습니다. 하지만 실제에서는 99% 확률로 맑습니다. 그렇다면 이 예보관의 예측을 실제의 날씨에 적용한다면 어떻게 될까요?

예보관의 예측에 따른 날씨 별 정보량은 다음과 같습니다.

- 맑음: ``-log₂(0.7) ≈ 0.51 비트``
- 흐림: ``-log₂(0.3) ≈ 1.73 비트``

실제 날씨에 해당 정보량을 적용하겠습니다.

``예보관의 예측을 실제 날씨에 적용했을 때의 평균 정보량``  
 `` = (실제 맑음 확률 × 잘못된 맑음 코드 길이) + (실제 흐림 확률 × 잘못된 흐림 코드 길이)``  
 `` = (0.99 × 0.51 비트) + (0.01 × 1.73 비트)``   
 `` ≈ 0.505 + 0.017``   
 `` = 0.522 비트``

실제 평균 정보량은 0.08 비트이지만 0.522 비트의 값이 나왔습니다. 해당 값을 우리는 크로스-엔트로피라고 합니다. 머신러닝에서 모델을 학습시킨다는 것은 이 크로스-엔트로피 값을 최소화하면서 실제 데이터의 분포와 비슷하게 만들어 나가는 것입니다.

#### KL 발산

KL 발산은 그래서 우리가 한 예측이 실제로는 얼마나 다른지, 분포와 분포간의 거리를 계산하는 것입니다. 다르게 말하면 낭비된 정보량을 표현한 것이라고 할 수 있습니다.

KL 발산은 크로스-엔트로피에서 엔트로피를 빼면 얻을 수 있습니다. 지금까지의 설명으로도 알 수 있듯이 KL 발산이 0이라면 실제 분포와 같은 것이고 KL 발산이 클수록 실제 분포와 다르다는 것을 예상할 수 있습니다.

``D_KL(P || Q) = 0.522 비트 - 0.08 비트 = 0.442 비트``

### 인과관계

조건부 확률은 유용한 통계적 해석을 제공하지만 인과관계를 추론할 때 함부로 사용하면 안됩니다. 인과관계는 데이터 분포의 변화에 강건한 예측모형을 만들 때에 필요합니다. 인과관계를 알기 위해서는 중첩요인 효과를 제거하고 원인에 해당하는 변수의 인과관계만을 계산해야 합니다.

![이미지](/assets/img/posts/boostcamp/day8/simpson.png)

위 그림에서 신장 결석이 작을 때와 클 때 각각의 경우에는 a의 치료법이 완치율이 높습니다. 그런데 전체에서 봤을 때에는 b가 완치율이 높습니다. 왜 이런 현상이 생기는 걸까요?

그 이유는 신장 결석의 크기의 영향을 고려하지 않았기 때문입니다. 신장 결석의 크기가 큰 사람들은 많은 사람들이 a 치료법을 골랐고, 작은 사람들은 b 치료법을 골랐습니다. 여기에서 다음과 같이 생각할 수 있습니다.

-> 각 부분의 확률이 크다고 해서 전체 확률이 큰 것은 아니다.
-> 치료법에 대한 완치율만 알고싶다면 신장 결석의 크기의 개입을 제거해야한다.

조정효과(intervention)을 통해 다음과 같이 Z의 개입을 제거할 수 있습니다.

``(a를 선택했을때 완치될 확률)``  
`` = (신장 결석이 작고 a로 완치될 확률) * (전체 인원 중 작은 신장결석을 가진 사람의 비율)``  
`` + (신장 결석이 크고 a로 완치될 확률) * (전체 인원 중 큰 신장결석을 가진 사람의 비율)``

같은 방법으로 b를 선택했을때 완치될 확률도 계산할 수 있습니다. 해당 방법으로 확률을 계산하게 된다면 a 치료법은 0.8325, b 치료법은 0.7789로 a 치료법의 완치 확률이 더 높은 것을 확인할 수 있습니다.

---
**피어세션을 통해 알아간 것**
- KL 발산에서 왜 P(x)를 곱하는지
  - 자주 일어나는 사건일수록 더 큰 가중치를 주어야 하기 때문에
- KL 발산은 음수가 될 수 없음

